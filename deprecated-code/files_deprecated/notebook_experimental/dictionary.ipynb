{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9daa8c83",
   "metadata": {},
   "source": [
    "## Create from manual terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2921dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR\\glossary.txt' # Replace with the path to your text file\n",
    "with open(file_path, 'r') as file: lines = file.readlines()\n",
    "glossary = [line.strip() for line in lines]\n",
    "# print(glossary[:10])\n",
    "\n",
    "file_path = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR\\target_topics.txt'\n",
    "with open(file_path, 'r') as file: lines = file.readlines()\n",
    "targets = [line.strip() for line in lines]\n",
    "# print(targets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66317034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abstract'], ['abstract', 'strategi', 'game'], ['accessori']]\n",
      "\n",
      "[['abstract'], ['abstract', 'strategy', 'game'], ['accessory']]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def lemmatize(token):\n",
    "    return lemmatizer.lemmatize(token, pos='v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def save_reference(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(lemmatize(token))\n",
    "    return result\n",
    "\n",
    "glossary_processed = [preprocess(doc) for doc in glossary]\n",
    "glossary_reference = [save_reference(doc) for doc in glossary]\n",
    "\n",
    "targets_processed = [preprocess(doc) for doc in targets]\n",
    "targets_reference = [save_reference(doc) for doc in targets]\n",
    "\n",
    "processed_docs = glossary_processed + targets_processed\n",
    "reference_docs = glossary_reference + targets_reference\n",
    "\n",
    "print(processed_docs[:3])\n",
    "print()\n",
    "\n",
    "# print(len(processed_docs))\n",
    "# print(len(reference_docs))\n",
    "# print()\n",
    "\n",
    "stem_ref = [stem for doc in processed_docs for stem in doc]\n",
    "lemm_ref = [lemma for doc in reference_docs for lemma in doc]\n",
    "\n",
    "print(reference_docs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6017898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_in_lists(a, b, target):\n",
    "    try:\n",
    "        index = a.index(target)\n",
    "        return b[index]\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdb855e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract 0\n",
      "game 1\n",
      "strategy 2\n",
      "accessory 3\n",
      "act 4\n",
      "action 5\n",
      "allowance 6\n",
      "point 7\n",
      "selection 8\n",
      "alpha 9\n",
      "player 10\n",
      "ameritrash 11\n",
      "analysis 12\n",
      "paralysis 13\n",
      "area 14\n",
      "control 15\n",
      "enclosure 16\n",
      "impulse 17\n",
      "movement 18\n",
      "arg 19\n",
      "auction 20\n",
      "bbg 21\n",
      "balance 22\n",
      "beer 23\n",
      "pretzels 24\n",
      "bet 25\n",
      "bgg 26\n",
      "patron 27\n",
      "bid 28\n",
      "bits 29\n",
      "blind 30\n",
      "block 31\n",
      "wargame 32\n",
      "bluff 33\n",
      "bpa 34\n",
      "break 35\n",
      "bsw 36\n",
      "card 37\n",
      "draft 38\n",
      "ccg 39\n",
      "cdg 40\n",
      "chit 41\n",
      "counter 42\n",
      "chrome 43\n",
      "closer 44\n",
      "coin 45\n",
      "collectible 46\n",
      "computational 47\n",
      "cooperative 48\n",
      "core 49\n",
      "group 50\n",
      "crayon 51\n",
      "rail 52\n",
      "crib 53\n",
      "sheet 54\n",
      "crt 55\n",
      "build 56\n",
      "deck 57\n",
      "designer 58\n",
      "dexterity 59\n",
      "dice 60\n",
      "die 61\n",
      "fest 62\n",
      "roll 63\n",
      "downtime 64\n",
      "crawl 65\n",
      "dungeon 66\n",
      "drm 67\n",
      "dry 68\n",
      "dudes 69\n",
      "map 70\n",
      "economic 71\n",
      "end 72\n",
      "euro 73\n",
      "eurogame 74\n",
      "eurodude 75\n",
      "eurotrash 76\n",
      "expansion 77\n",
      "experience 78\n",
      "family 79\n",
      "factor 80\n",
      "fart 81\n",
      "fiddly 82\n",
      "filler 83\n",
      "flgs 84\n",
      "friendly 85\n",
      "tie 86\n",
      "abbreviations 87\n",
      "gameid 88\n",
      "gamer 89\n",
      "gamey 90\n",
      "gateway 91\n",
      "gbg 92\n",
      "geek 93\n",
      "geekbadge 94\n",
      "geekbuddy 95\n",
      "geekcoin 96\n",
      "geekgold 97\n",
      "geeklists 98\n",
      "geekmod 99\n",
      "week 100\n",
      "german 101\n",
      "table 102\n",
      "go 103\n",
      "nuclear 104\n",
      "golden 105\n",
      "thumb 106\n",
      "gotw 107\n",
      "graphical 108\n",
      "representation 109\n",
      "user 110\n",
      "grognard 111\n",
      "gur 112\n",
      "think 113\n",
      "heavy 114\n",
      "heft 115\n",
      "hex 116\n",
      "irl 117\n",
      "jase 118\n",
      "kingmaker 119\n",
      "larp 120\n",
      "leach 121\n",
      "leech 122\n",
      "legacy 123\n",
      "lcg 124\n",
      "live 125\n",
      "light 126\n",
      "log 127\n",
      "play 128\n",
      "luck 129\n",
      "majority 130\n",
      "market 131\n",
      "mass 132\n",
      "meaty 133\n",
      "mechanism 134\n",
      "meeples 135\n",
      "metagame 136\n",
      "list 137\n",
      "meta 138\n",
      "metalist 139\n",
      "microbadge 140\n",
      "max 141\n",
      "mini 142\n",
      "miniatures 143\n",
      "multiplayer 144\n",
      "negotiation 145\n",
      "nerd 146\n",
      "nis 147\n",
      "newbie 148\n",
      "ngf 149\n",
      "non 150\n",
      "olgs 151\n",
      "oop 152\n",
      "oos 153\n",
      "opener 154\n",
      "operational 155\n",
      "overanalyze 156\n",
      "overtext 157\n",
      "parakeetitis 158\n",
      "conflict 159\n",
      "parasitic 160\n",
      "party 161\n",
      "paste 162\n",
      "theme 163\n",
      "badge 164\n",
      "pbem 165\n",
      "information 166\n",
      "perfect 167\n",
      "diplomacy 168\n",
      "petty 169\n",
      "pie 170\n",
      "rule 171\n",
      "interaction 172\n",
      "playtest 173\n",
      "plonk 174\n",
      "pnp 175\n",
      "salad 176\n",
      "creep 177\n",
      "power 178\n",
      "press 179\n",
      "processional 180\n",
      "publisher 181\n",
      "punch 182\n",
      "pvp 183\n",
      "quarterback 184\n",
      "quickbar 185\n",
      "quicklink 186\n",
      "quickpage 187\n",
      "race 188\n",
      "random 189\n",
      "raw 190\n",
      "creation 191\n",
      "reference 192\n",
      "replay 193\n",
      "value 194\n",
      "rgb 195\n",
      "paper 196\n",
      "rock 197\n",
      "scissor 198\n",
      "rpg 199\n",
      "rsp 200\n",
      "lawyer 201\n",
      "sbw 202\n",
      "scenario 203\n",
      "deployment 204\n",
      "secret 205\n",
      "unit 206\n",
      "set 207\n",
      "simulation 208\n",
      "casualty 209\n",
      "sleeve 210\n",
      "solvable 211\n",
      "spielfreak 212\n",
      "sprue 213\n",
      "losses 214\n",
      "step 215\n",
      "tableau 216\n",
      "builder 217\n",
      "tactics 218\n",
      "tag 219\n",
      "tcg 220\n",
      "tec 221\n",
      "territory 222\n",
      "teutonic 223\n",
      "thematic 224\n",
      "lay 225\n",
      "tile 226\n",
      "train 227\n",
      "take 228\n",
      "trick 229\n",
      "troll 230\n",
      "turtle 231\n",
      "develop 232\n",
      "variant 233\n",
      "condition 234\n",
      "victory 235\n",
      "wager 236\n",
      "waro 237\n",
      "weuro 238\n",
      "wbc 239\n",
      "wiki 240\n",
      "wip 241\n",
      "wantlist 242\n",
      "wishlist 243\n",
      "placement 244\n",
      "worker 245\n",
      "sum 246\n",
      "zero 247\n",
      "zoc 248\n",
      "abstruse 249\n",
      "conceptual 250\n",
      "summary 251\n",
      "theoretical 252\n",
      "intellectual 253\n",
      "mind 254\n",
      "pure 255\n",
      "strategic 256\n",
      "add 257\n",
      "attachment 258\n",
      "enhancement 259\n",
      "extension 260\n",
      "supplement 261\n",
      "character 262\n",
      "dramatic 263\n",
      "improvisation 264\n",
      "role 265\n",
      "theatrical 266\n",
      "activation 267\n",
      "apas 268\n",
      "energy 269\n",
      "turn 270\n",
      "choice 271\n",
      "decision 272\n",
      "make 273\n",
      "plan 274\n",
      "boss 275\n",
      "dictator 276\n",
      "dominant 277\n",
      "leader 278\n",
      "american 279\n",
      "drive 280\n",
      "immersive 281\n",
      "style 282\n",
      "fatigue 283\n",
      "gridlock 284\n",
      "overload 285\n",
      "overthinking 286\n",
      "conquest 287\n",
      "influence 288\n",
      "region 289\n",
      "containment 290\n",
      "land 291\n",
      "space 292\n",
      "zone 293\n",
      "location 294\n",
      "alternate 295\n",
      "bend 296\n",
      "puzzle 297\n",
      "reality 298\n",
      "solve 299\n",
      "story 300\n",
      "base 301\n",
      "sale 302\n",
      "board 303\n",
      "boardgamegeek 304\n",
      "community 305\n",
      "database 306\n",
      "enthusiasts 307\n",
      "hub 308\n",
      "equality 309\n",
      "equilibrium 310\n",
      "fairness 311\n",
      "stability 312\n",
      "symmetry 313\n",
      "casual 314\n",
      "easy 315\n",
      "hearted 316\n",
      "relax 317\n",
      "social 318\n",
      "ante 319\n",
      "gamble 320\n",
      "place 321\n",
      "stake 322\n",
      "alea 323\n",
      "elements 324\n",
      "independent 325\n",
      "intervention 326\n",
      "introduce 327\n",
      "outside 328\n",
      "fortune 329\n",
      "chance 330\n",
      "serendipity 331\n",
      "fate 332\n",
      "providence 333\n",
      "destiny 334\n",
      "bless 335\n",
      "opportunity 336\n",
      "fluke 337\n",
      "stroke 338\n",
      "windfall 339\n",
      "success 340\n",
      "prosperity 341\n",
      "auspiciousness 342\n",
      "win 343\n",
      "bonanza 344\n",
      "serendipitous 345\n",
      "happy 346\n",
      "lucky 347\n",
      "risk 348\n",
      "hazard 349\n",
      "fortuity 350\n",
      "uncertainty 351\n",
      "probability 352\n",
      "venture 353\n",
      "coincidence 354\n",
      "lottery 355\n",
      "arbitrary 356\n",
      "unpredictable 357\n",
      "haphazard 358\n",
      "unplanned 359\n",
      "indiscriminate 360\n",
      "accidental 361\n",
      "fortuitous 362\n",
      "unexpected 363\n",
      "whimsical 364\n",
      "erratic 365\n",
      "hit 366\n",
      "miss 367\n",
      "unsystematic 368\n",
      "chaotic 369\n",
      "unforeseen 370\n",
      "coincidental 371\n",
      "spontaneous 372\n",
      "irregular 373\n",
      "access 374\n",
      "automatic 375\n",
      "bookkeeping 376\n",
      "continuosly 377\n",
      "data 378\n",
      "include 379\n",
      "manual 380\n",
      "need 381\n",
      "potentially 382\n",
      "process 383\n",
      "record 384\n",
      "rulebook 385\n",
      "semi 386\n",
      "management 387\n",
      "resource 388\n",
      "track 389\n",
      "scorekeeping 390\n",
      "asset 391\n",
      "currency 392\n",
      "inventory 393\n",
      "financial 394\n",
      "keep 395\n",
      "allocation 396\n",
      "transaction 397\n",
      "income 398\n",
      "expense 399\n",
      "account 400\n",
      "cash 401\n",
      "flow 402\n",
      "statements 403\n",
      "budget 404\n",
      "guide 405\n",
      "instruction 406\n",
      "gameplay 407\n",
      "regulations 408\n",
      "handbook 409\n",
      "companion 410\n",
      "compendium 411\n",
      "encyclopedia 412\n",
      "digest 413\n",
      "have 414\n",
      "little 415\n",
      "mean 416\n",
      "time 417\n",
      "unproductive 418\n",
      "wait 419\n",
      "rest 420\n",
      "pause 421\n",
      "respite 422\n",
      "intermission 423\n",
      "interval 424\n",
      "breather 425\n",
      "leisure 426\n",
      "lull 427\n",
      "reprieve 428\n",
      "recreation 429\n",
      "unwind 430\n",
      "refreshment 431\n",
      "regeneration 432\n",
      "period 433\n",
      "free 434\n",
      "recess 435\n",
      "tedious 436\n",
      "dull 437\n",
      "monotonous 438\n",
      "uninteresting 439\n",
      "tiresome 440\n",
      "mundane 441\n",
      "repetitive 442\n",
      "lackluster 443\n",
      "drab 444\n",
      "insipid 445\n",
      "flat 446\n",
      "lifeless 447\n",
      "unexciting 448\n",
      "stale 449\n",
      "tire 450\n",
      "wearisome 451\n",
      "lame 452\n",
      "banal 453\n",
      "hum 454\n",
      "degree 455\n",
      "participants 456\n",
      "communication 457\n",
      "engagement 458\n",
      "connection 459\n",
      "involvement 460\n",
      "collaboration 461\n",
      "interplay 462\n",
      "association 463\n",
      "exchange 464\n",
      "relationship 465\n",
      "interconnection 466\n",
      "reciprocity 467\n",
      "interrelation 468\n",
      "inclusion 469\n",
      "interchange 470\n",
      "impact 471\n",
      "effect 472\n",
      "authority 473\n",
      "sway 474\n",
      "persuasion 475\n",
      "leverage 476\n",
      "clout 477\n",
      "manipulation 478\n",
      "supremacy 479\n",
      "ascendancy 480\n",
      "guidance 481\n",
      "command 482\n",
      "pressure 483\n",
      "dominion 484\n",
      "governance 485\n",
      "hold 486\n",
      "sovereignty 487\n",
      "advantage 488\n",
      "arise 489\n",
      "bash 490\n",
      "benefit 491\n",
      "case 492\n",
      "conduct 493\n",
      "curb 494\n",
      "detriment 495\n",
      "directly 496\n",
      "force 497\n",
      "gain 498\n",
      "let 499\n",
      "prevent 500\n",
      "sacrifice 501\n",
      "situation 502\n",
      "unfortunate 503\n",
      "chief 504\n",
      "head 505\n",
      "director 506\n",
      "captain 507\n",
      "supervisor 508\n",
      "executive 509\n",
      "president 510\n",
      "ceo 511\n",
      "officer 512\n",
      "principal 513\n",
      "superintendent 514\n",
      "team 515\n",
      "champion 516\n",
      "ruler 517\n",
      "mentor 518\n",
      "trailblazer 519\n",
      "unavoidable 520\n",
      "certain 521\n",
      "imminent 522\n",
      "inescapable 523\n",
      "bind 524\n",
      "happen 525\n",
      "fat 526\n",
      "irrevocable 527\n",
      "unpreventable 528\n",
      "determine 529\n",
      "inevitable 530\n",
      "predestine 531\n",
      "sure 532\n",
      "decree 533\n",
      "fix 534\n",
      "unalterable 535\n",
      "unquestionable 536\n",
      "settle 537\n",
      "necessitate 538\n",
      "indisputable 539\n",
      "offer 540\n",
      "surrender 541\n",
      "give 542\n",
      "renunciation 543\n",
      "forfeit 544\n",
      "abandonment 545\n",
      "selflessness 546\n",
      "devotion 547\n",
      "martyrdom 548\n",
      "donation 549\n",
      "submission 550\n",
      "disposal 551\n",
      "abnegation 552\n",
      "immolation 553\n",
      "dedication 554\n",
      "expenditure 555\n",
      "self 556\n",
      "abjuration 557\n",
      "relinquishment 558\n",
      "complex 559\n",
      "complicate 560\n",
      "difficult 561\n",
      "equip 562\n",
      "exceptions 563\n",
      "immediate 564\n",
      "lead 565\n",
      "learn 566\n",
      "master 567\n",
      "predictable 568\n",
      "problem 569\n",
      "qualitatively 570\n",
      "quantitatively 571\n",
      "result 572\n",
      "understand 573\n",
      "variables 574\n",
      "repercussions 575\n",
      "intricate 576\n",
      "elaborate 577\n",
      "sophisticate 578\n",
      "faceted 579\n",
      "multi 580\n",
      "detail 581\n",
      "confuse 582\n",
      "dense 583\n",
      "knotty 584\n",
      "challenge 585\n",
      "confound 586\n",
      "byzantine 587\n",
      "intrigue 588\n",
      "nuanced 589\n",
      "perplex 590\n",
      "ambiguous 591\n",
      "convolute 592\n",
      "tangle 593\n",
      "hard 594\n",
      "troublesome 595\n",
      "daunt 596\n",
      "arduous 597\n",
      "thorny 598\n",
      "expertise 599\n",
      "proficiency 600\n",
      "competence 601\n",
      "ability 602\n",
      "talent 603\n",
      "mastery 604\n",
      "aptitude 605\n",
      "capability 606\n",
      "craftsmanship 607\n",
      "know 608\n",
      "skillfulness 609\n",
      "prowess 610\n",
      "gift 611\n",
      "savvy 612\n",
      "technique 613\n",
      "knack 614\n",
      "flair 615\n",
      "artistry 616\n",
      "acumen 617\n",
      "twist 618\n",
      "tortuous 619\n",
      "circuitous 620\n",
      "serpentine 621\n",
      "weave 622\n",
      "structure 623\n",
      "labyrinthine 624\n",
      "meander 625\n",
      "layer 626\n",
      "tough 627\n",
      "demand 628\n",
      "laborious 629\n",
      "strenuous 630\n",
      "try 631\n",
      "tricky 632\n",
      "problematic 633\n",
      "formidable 634\n",
      "rigorous 635\n",
      "herculean 636\n",
      "test 637\n",
      "struggle 638\n",
      "burdensome 639\n",
      "tax 640\n",
      "acquire 641\n",
      "knowledge 642\n",
      "absorb 643\n",
      "grasp 644\n",
      "comprehend 645\n",
      "study 646\n",
      "educate 647\n",
      "discover 648\n",
      "adapt 649\n",
      "pick 650\n",
      "improve 651\n",
      "familiarize 652\n",
      "explore 653\n",
      "assimilate 654\n",
      "bore 655\n",
      "slog 656\n",
      "dreary 657\n",
      "numb 658\n",
      "long 659\n",
      "wind 660\n",
      "protract 661\n",
      "irksome 662\n",
      "plod 663\n",
      "pace 664\n",
      "slow 665\n",
      "simple 666\n",
      "effortless 667\n",
      "straightforward 668\n",
      "uncomplicated 669\n",
      "basic 670\n",
      "undemanding 671\n",
      "painless 672\n",
      "smooth 673\n",
      "trouble 674\n",
      "elementary 675\n",
      "breezy 676\n",
      "cake 677\n",
      "piece 678\n",
      "park 679\n",
      "walk 680\n",
      "sweat 681\n",
      "unchallenging 682\n",
      "seamless 683\n",
      "quick 684\n",
      "captivate 685\n",
      "enthral 686\n",
      "grip 687\n",
      "compel 688\n",
      "interest 689\n",
      "fascinate 690\n",
      "entertain 691\n",
      "stimulate 692\n",
      "charm 693\n",
      "enchant 694\n",
      "delightful 695\n",
      "mesmerize 696\n",
      "allure 697\n",
      "spellbind 698\n",
      "excite 699\n",
      "provoke 700\n",
      "commitment 701\n",
      "enthusiasm 702\n",
      "contribution 703\n",
      "empowerment 704\n",
      "presence 705\n",
      "variation 706\n",
      "edition 707\n",
      "release 708\n",
      "iteration 709\n",
      "form 710\n",
      "interpretation 711\n",
      "rendition 712\n",
      "revision 713\n",
      "modification 714\n",
      "update 715\n",
      "upgrade 716\n",
      "growth 717\n",
      "enlargement 718\n",
      "increase 719\n",
      "augmentation 720\n",
      "progress 721\n",
      "advancement 722\n",
      "scale 723\n",
      "amplification 724\n",
      "broaden 725\n",
      "spread 726\n",
      "evolvement 727\n",
      "escalation 728\n",
      "upsizing 729\n",
      "upsurge 730\n",
      "diversification 731\n",
      "addition 732\n",
      "annexation 733\n",
      "appendage 734\n",
      "extra 735\n",
      "accrual 736\n",
      "increment 737\n",
      "bonus 738\n",
      "furtherance 739\n",
      "plus 740\n",
      "single 741\n",
      "solitary 742\n",
      "individual 743\n",
      "solo 744\n",
      "lone 745\n",
      "mode 746\n",
      "adventure 747\n",
      "solitaire 748\n",
      "campaign 749\n",
      "man 750\n",
      "solus 751\n",
      "isolate 752\n",
      "work 753\n",
      "effort 754\n",
      "joint 755\n",
      "mutual 756\n",
      "partnership 757\n",
      "synergy 758\n",
      "combine 759\n",
      "unify 760\n",
      "ally 761\n",
      "endeavor 762\n",
      "share 763\n",
      "communal 764\n",
      "consort 765\n",
      "harmony 766\n",
      "proportion 767\n",
      "evenness 768\n",
      "counterbalance 769\n",
      "poise 770\n",
      "composure 771\n",
      "moderation 772\n",
      "consistency 773\n",
      "rectitude 774\n",
      "impartiality 775\n",
      "neutrality 776\n",
      "steadiness 777\n",
      "accord 778\n",
      "serenity 779\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)  \n",
    "dictionary.save('dictionary_file')\n",
    "dictionary = Dictionary.load('dictionary_file')\n",
    "for token, idx in dictionary.token2id.items(): print(find_element_in_lists(stem_ref, lemm_ref, token), idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5179e8",
   "metadata": {},
   "source": [
    "## Extract from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e0718c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "df = pd.read_csv(os.path.join(path_original_data,'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "string_list = df['gensim_comment_verbs'].tolist()\n",
    "comments_list = [ast.literal_eval(s) for s in string_list]\n",
    "\n",
    "# comments_list = []\n",
    "# for s in string_list: comments_list.append(ast.literal_eval(s))\n",
    "# print(comments_list[:2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc17c8",
   "metadata": {},
   "source": [
    "Original size:  37392\n",
    "\n",
    "Trimmed size:  1740 \n",
    "\n",
    "Trimmed size:  1589 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2be4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_elements(list1, list2):\n",
    "    unique_elements_list1 = list(set(list1) - set(list2))\n",
    "    unique_elements_list2 = list(set(list2) - set(list1))\n",
    "    unique_elements = unique_elements_list1 + unique_elements_list2\n",
    "    return unique_elements\n",
    "\n",
    "def write_words_to_file(word_list, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for word in word_list:\n",
    "            file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e58794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size:  34764\n",
      "Trimmed size:  1436 \n",
      "\n",
      "\n",
      "Original size:  1436 \n",
      "\n",
      "Trimmed size:  1325 \n",
      "\n",
      "Dictionary(1325 unique tokens: ['abil', 'abl', 'absolut', 'abstract', 'access']...)\n"
     ]
    }
   ],
   "source": [
    "auxiliar = gensim.corpora.Dictionary(comments_list) \n",
    "print('Original size: ',len(auxiliar))\n",
    "list_a = [token for token, idx in auxiliar.token2id.items()]\n",
    "    \n",
    "auxiliar.filter_extremes(no_below=100, no_above=0.5, keep_n= None)\n",
    "print('Trimmed size: ',len(auxiliar), '\\n')\n",
    "list_b = [token for token, idx in auxiliar.token2id.items()]\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "tokens = [token for token in auxiliar.values()]\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "filtered_tokens = [token for token, pos_tag in zip(tokens, pos_tags) if pos_tag[1] not in verbs]\n",
    "filtered_dictionary = corpora.Dictionary()\n",
    "filtered_dictionary.doc2bow(filtered_tokens, allow_update=True)\n",
    "\n",
    "print('Original size: ',len(auxiliar))\n",
    "print('Trimmed size: ',len(filtered_dictionary), '\\n')\n",
    "list_c = [token for token, idx in filtered_dictionary.token2id.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49b9aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in find_unique_elements(list_a, list_b): print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d45226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in find_unique_elements(list_b, list_c): print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc8e6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "auxiliar.save('auxiliar_dictionary_file')\n",
    "auxiliar = Dictionary.load('auxiliar_dictionary_file')\n",
    "# for token, idx in auxiliar.token2id.items(): print(token, idx)\n",
    "    \n",
    "write_words_to_file([token for token, idx in auxiliar.token2id.items()], 'auxiliary_list.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b3f1e",
   "metadata": {},
   "source": [
    "## Fine tune the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4e84cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words_from_file(filename):\n",
    "    word_list = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fd1fc03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length:  1865\n",
      "New length:  1867 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original length: ', len(dictionary))\n",
    "dictionary.merge_with(auxiliar)\n",
    "print('New length: ', len(dictionary), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b163ec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'card', 'great', 'like', 'good', 'board', 'fun', 'game', 'play', 'player'] \n",
      "\n",
      "Dictionary(1860 unique tokens: ['abstract', 'strategi', 'accessori', 'act', 'action']...)\n",
      "Dictionary(1858 unique tokens: ['abstract', 'strategi', 'accessori', 'act', 'action']...) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'words_to_remove.txt'\n",
    "words_to_remove = read_words_from_file(filename)\n",
    "print(words_to_remove, '\\n')\n",
    "print(dictionary)\n",
    "word_ids = [dictionary.token2id[word] for word in words_to_remove if word in dictionary.token2id]\n",
    "dictionary.filter_tokens(bad_ids=word_ids)\n",
    "dictionary.compactify()\n",
    "# for word_id in word_ids: dictionary.token2id.popitem(word_id)\n",
    "print(dictionary, '\\n')\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary.save('final_dictionary_file')\n",
    "dictionary = Dictionary.load('final_dictionary_file')\n",
    "# for token, idx in auxiliar.token2id.items(): print(token, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c2997",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759cc2c",
   "metadata": {},
   "source": [
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8534395/\n",
    "\n",
    "https://jmlr.csail.mit.edu/papers/volume21/20-079/20-079.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
