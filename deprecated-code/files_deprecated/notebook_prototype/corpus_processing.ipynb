{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc31a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "df = pd.read_csv(os.path.join(path_original_data,'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "string_list = df['gensim_comment_verbs'].tolist()\n",
    "comments_list = [ast.literal_eval(s) for s in string_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7002310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_elements(list1, list2):\n",
    "    unique_elements_list1 = list(set(list1) - set(list2))\n",
    "    unique_elements_list2 = list(set(list2) - set(list1))\n",
    "    unique_elements = unique_elements_list1 + unique_elements_list2\n",
    "    return unique_elements\n",
    "\n",
    "def write_words_to_file(word_list, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for word in word_list:\n",
    "            file.write(word + '\\n')\n",
    "            \n",
    "def read_words_from_file(filename):\n",
    "    word_list = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "    return word_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fddb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size:  34764\n",
      "Trimmed size:  121 \n",
      "\n",
      "Original size:  121\n",
      "Trimmed size:  112 \n",
      "\n",
      "absolut\n",
      "action\n",
      "actual\n",
      "amaz\n",
      "art\n",
      "artwork\n",
      "bad\n",
      "base\n",
      "beauti\n",
      "best\n",
      "better\n",
      "big\n",
      "bird\n",
      "bite\n",
      "board\n",
      "box\n",
      "build\n",
      "builder\n",
      "campaign\n",
      "card\n",
      "charact\n",
      "choic\n",
      "collect\n",
      "combat\n",
      "complex\n",
      "compon\n",
      "decis\n",
      "deck\n",
      "definit\n",
      "design\n",
      "differ\n",
      "easi\n",
      "edit\n",
      "end\n",
      "engin\n",
      "enjoy\n",
      "euro\n",
      "excel\n",
      "expans\n",
      "faction\n",
      "famili\n",
      "fantast\n",
      "far\n",
      "favorit\n",
      "friend\n",
      "fun\n",
      "game\n",
      "gameplay\n",
      "gloomhaven\n",
      "good\n",
      "great\n",
      "group\n",
      "hand\n",
      "hard\n",
      "high\n",
      "hour\n",
      "interact\n",
      "interest\n",
      "level\n",
      "like\n",
      "littl\n",
      "long\n",
      "lot\n",
      "luck\n",
      "manag\n",
      "mar\n",
      "mayb\n",
      "mechan\n",
      "new\n",
      "nice\n",
      "option\n",
      "overal\n",
      "peopl\n",
      "perfect\n",
      "person\n",
      "placement\n",
      "player\n",
      "possibl\n",
      "power\n",
      "pretti\n",
      "probabl\n",
      "product\n",
      "puzzl\n",
      "qualiti\n",
      "quick\n",
      "random\n",
      "rat\n",
      "replay\n",
      "resourc\n",
      "right\n",
      "round\n",
      "rule\n",
      "scenario\n",
      "score\n",
      "setup\n",
      "simpl\n",
      "solid\n",
      "solo\n",
      "space\n",
      "stori\n",
      "strategi\n",
      "sure\n",
      "tabl\n",
      "theme\n",
      "thing\n",
      "tile\n",
      "time\n",
      "uniqu\n",
      "version\n",
      "way\n",
      "worker\n",
      "year\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "auxiliar = gensim.corpora.Dictionary(comments_list) \n",
    "print('Original size: ',len(auxiliar))\n",
    "list_a = [token for token, idx in auxiliar.token2id.items()]\n",
    "    \n",
    "auxiliar.filter_extremes(no_below=len(auxiliar)*0.05, no_above=0.99, keep_n= None)\n",
    "print('Trimmed size: ',len(auxiliar), '\\n')\n",
    "list_b = [token for token, idx in auxiliar.token2id.items()]\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "tokens = [token for token in auxiliar.values()]\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "filtered_tokens = [token for token, pos_tag in zip(tokens, pos_tags) if pos_tag[1] not in verbs]\n",
    "filtered_dictionary = corpora.Dictionary()\n",
    "filtered_dictionary.doc2bow(filtered_tokens, allow_update=True)\n",
    "\n",
    "print('Original size: ',len(auxiliar))\n",
    "print('Trimmed size: ',len(filtered_dictionary), '\\n')\n",
    "list_c = [token for token, idx in filtered_dictionary.token2id.items()]\n",
    "\n",
    "for word in list_c:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace74fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'player', 'play', 'card', 'like', 'fun', 'time', 'mechan'] \n",
      "\n",
      "112\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "filename = 'words_to_remove.txt'\n",
    "words_to_remove = read_words_from_file(filename)\n",
    "\n",
    "print(words_to_remove, '\\n')\n",
    "print(len(filtered_dictionary))\n",
    "\n",
    "word_ids = [filtered_dictionary.token2id[word] for word in words_to_remove if word in filtered_dictionary.token2id]\n",
    "filtered_dictionary.filter_tokens(bad_ids=word_ids)\n",
    "filtered_dictionary.compactify()\n",
    "\n",
    "print(len(filtered_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9277c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "filtered_dictionary.save('corpus_dictionary')\n",
    "filtered_dictionary = Dictionary.load('corpus_dictionary')\n",
    "# write_words_to_file([token for token, idx in auxiliar.token2id.items()], 'auxiliary_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049b60c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
