{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b5565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def extract_keys_and_elements(json_data):\n",
    "    keys = []\n",
    "    elements = []\n",
    "    \n",
    "    def traverse(data):\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                keys.append(key)\n",
    "                traverse(value)\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                traverse(item)\n",
    "        else:\n",
    "            elements.append(data)\n",
    "    \n",
    "    traverse(json_data)\n",
    "    return keys, elements\n",
    "\n",
    "prefix = 'TOPIC_'\n",
    "topic_list = ['luck', 'downtime', 'leader', 'bookeeping', 'complicated', 'complex', 'negative', 'extras']\n",
    "word_list = []\n",
    "\n",
    "for topic in topic_list:\n",
    "    \n",
    "    filename = os.path.join('TOPICS', prefix + topic + '.json')\n",
    "\n",
    "    with open(filename, 'r') as file: json_data = json.load(file)\n",
    "\n",
    "    keys, elements = extract_keys_and_elements(json_data)\n",
    "    word_list = word_list + keys + elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95dfb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2308\n",
      "1420\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "processed_docs = []\n",
    "reference_sheet = {}\n",
    "for doc in word_list:\n",
    "    processed_words = preprocess(doc)\n",
    "    processed_docs.append(processed_words)\n",
    "    for word in processed_words:\n",
    "        if word in reference_sheet: reference_sheet[word].append(doc)\n",
    "        else: reference_sheet[word] = [doc]\n",
    "reference_sheet = {key: list(set(values)) for key, values in reference_sheet.items()}                      \n",
    "            \n",
    "print(len(processed_docs))\n",
    "print(len(reference_sheet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ba455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)  \n",
    "dictionary.save('dictionary')\n",
    "dictionary = Dictionary.load('dictionary')\n",
    "# for token, idx in dictionary.token2id.items(): print(token, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1e0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'player', 'play', 'card', 'like'] \n",
      "\n",
      "1420\n",
      "1416 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_words_from_file(filename):\n",
    "    word_list = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "filename = 'words_to_remove.txt'\n",
    "words_to_remove = read_words_from_file(filename)\n",
    "\n",
    "print(words_to_remove, '\\n')\n",
    "print(len(dictionary))\n",
    "\n",
    "word_ids = [dictionary.token2id[word] for word in words_to_remove if word in dictionary.token2id]\n",
    "dictionary.filter_tokens(bad_ids=word_ids)\n",
    "dictionary.compactify()\n",
    "# for word_id in word_ids: dictionary.token2id.popitem(word_id)\n",
    "\n",
    "print(len(dictionary), '\\n')\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary.save('trimmed_dictionary')\n",
    "dictionary = Dictionary.load('trimmed_dictionary')\n",
    "# for token, idx in auxiliar.token2id.items(): print(token, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be0b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_data = json.dumps(reference_sheet)\n",
    "with open('reference_sheet_manual.json', 'w') as file: file.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
