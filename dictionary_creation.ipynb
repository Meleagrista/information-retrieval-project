{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b23347",
   "metadata": {},
   "source": [
    "# READING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49efa963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Function to find unique elements in two lists\n",
    "def find_unique_elements(list1, list2):\n",
    "    unique_elements_list1 = list(set(list1) - set(list2))\n",
    "    unique_elements_list2 = list(set(list2) - set(list1))\n",
    "    unique_elements = unique_elements_list1 + unique_elements_list2\n",
    "    return unique_elements\n",
    "\n",
    "# Function to write words to a file\n",
    "def write_words_to_file(word_list, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for word in word_list:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "# Function to read words from a file\n",
    "def read_words_from_file(filename):\n",
    "    word_list = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "# Path to the original data file\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "\n",
    "# Read the pre-processed data file into a DataFrame\n",
    "df = pd.read_csv(os.path.join(path_original_data, 'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "\n",
    "# Convert string representation of lists to actual lists\n",
    "string_list = df['gensim_comment_verbs'].tolist()\n",
    "comments_list = [ast.literal_eval(s) for s in string_list]\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary from the comments list\n",
    "auxiliar = gensim.corpora.Dictionary(comments_list)\n",
    "print('Corpus size: ', len(auxiliar))\n",
    "list_a = [token for token, idx in auxiliar.token2id.items()]\n",
    "\n",
    "# Filter the dictionary based on word frequencies\n",
    "# Remove words that appear in less than 5% of the comments or more than 99% of the comments\n",
    "auxiliar.filter_extremes(no_below=len(auxiliar) * 0.05, no_above=0.99, keep_n=None)\n",
    "print('Filtered corpus size: ', len(auxiliar))\n",
    "list_b = [token for token, idx in auxiliar.token2id.items()]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "tokens = [token for token in auxiliar.values()]\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Filter out tokens that are verbs\n",
    "filtered_tokens = [token for token, pos_tag in zip(tokens, pos_tags) if pos_tag[1] not in verbs]\n",
    "filtered_dictionary = corpora.Dictionary()\n",
    "filtered_dictionary.doc2bow(filtered_tokens, allow_update=True)\n",
    "\n",
    "print('Corpus without verbs size: ', len(filtered_dictionary), '\\n')\n",
    "list_c = [token for token, idx in filtered_dictionary.token2id.items()]\n",
    "\n",
    "filename = 'words_to_remove.txt'\n",
    "\n",
    "# Read the list of words to remove from a file\n",
    "words_to_remove = read_words_from_file(filename)\n",
    "\n",
    "print(words_to_remove, '\\n')\n",
    "word_ids = [filtered_dictionary.token2id[word] for word in words_to_remove if word in filtered_dictionary.token2id]\n",
    "\n",
    "# Filter the dictionary by removing specific words\n",
    "filtered_dictionary.filter_tokens(bad_ids=word_ids)\n",
    "filtered_dictionary.compactify()\n",
    "\n",
    "print('Revised corpus size: ', len(filtered_dictionary))\n",
    "\n",
    "dictionary = filtered_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9868027",
   "metadata": {},
   "source": [
    "# GETTING IDEAS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd8678dc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Select the value of 'n' to determine the index of the comment to be processed\n",
    "n = 1\n",
    "\n",
    "# Sort the DataFrame 'df' based on the 'word_count' column in descending order\n",
    "sorted_df = df.sort_values(by='word_count', ascending=False)\n",
    "\n",
    "# Get the first ten comments from the sorted DataFrame\n",
    "first_ten_comments = list(sorted_df['comment'].head(10))\n",
    "\n",
    "# Select the comment at index 'n' from the first ten comments\n",
    "text = first_ten_comments[n]\n",
    "\n",
    "# Define a list of symbols to be replaced in the comment text\n",
    "symbol_list = ['◆', '▼', '▲', '[u]']\n",
    "\n",
    "# Iterate over each symbol in the symbol list\n",
    "for symbol in symbol_list:\n",
    "    # Escape the symbol using re.escape() to ensure literal matching\n",
    "    pattern = re.escape(symbol)\n",
    "    # Replace the symbol with '\\n\\n- ' (two newlines followed by a hyphen)\n",
    "    text = re.sub(pattern, '\\n\\n- ', text)\n",
    "\n",
    "# Remove the remaining '[/COLOR][/BGCOLOR] [b]' string from the modified comment text\n",
    "modified_text = text.replace('[/COLOR][/BGCOLOR] [b]', '')\n",
    "\n",
    "# Print the modified comment text\n",
    "print(modified_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a41bf6",
   "metadata": {},
   "source": [
    "# SPECIFYING TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def generate_synonyms(word_list, num_synonyms, full_return=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Generate synonyms for a given list of words using WordNet.\n",
    "\n",
    "    Args:\n",
    "        word_list (list): List of words.\n",
    "        num_synonyms (int): Number of synonyms to generate per word.\n",
    "        full_return (bool, optional): Whether to return all synonyms or a subset based on 'num_synonyms'. Defaults to False.\n",
    "        verbose (bool, optional): Whether to print the generated synonyms. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: List of generated synonyms.\n",
    "    \"\"\"\n",
    "    synonyms = []\n",
    "    \n",
    "    # Iterate over each word in the word_list\n",
    "    for word in word_list:\n",
    "        # Get the synsets (sets of synonymous words) for the current word\n",
    "        synsets = wordnet.synsets(word)\n",
    "        \n",
    "        # Iterate over each synset\n",
    "        for synset in synsets:\n",
    "            # Extend the synonyms list with the lemma names of the synset\n",
    "            synonyms.extend(synset.lemma_names())\n",
    "    \n",
    "    # Remove duplicates from the synonyms list\n",
    "    synonyms = list(set(synonyms))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Generated synonyms: {synonyms}\\n\")\n",
    "    \n",
    "    if full_return:\n",
    "        return synonyms\n",
    "    else:\n",
    "        # Return a subset of the synonyms list based on the 'num_synonyms' argument\n",
    "        # If 'num_synonyms' is greater than the total number of synonyms, return all synonyms\n",
    "        return synonyms[:num_synonyms] if num_synonyms < len(synonyms) else synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target topics dictionary\n",
    "target_topics = {}\n",
    "\n",
    "# BOOKEEPING\n",
    "\n",
    "# Actions\n",
    "string1 = \"adress, bookeeping, tracking, writing, documenting, recording, organizing, remembering, looking, reading, understanding, calculate\"\n",
    "# Components\n",
    "string2= \"rule set, rulebook, handbook, guidebook, guide, reference, knowledge, text, information, data, notes, progress\"\n",
    "# Negative connotations\n",
    "string3 = \"excesive rules, vague, tedious, confusing, slow, time-consuming, methodical, difficult, hard, long, endless\"\n",
    "# Positive connotations\n",
    "string4 = \"simple, short, easy, understandable, helpful, organized, quick setup, remember, little, few, illustarted\"\n",
    "# Other related terms\n",
    "string5 = \"math, mathematics\"\n",
    "\n",
    "string_list = [string1, string2, string3, string4, string5]\n",
    "synonym_list = []\n",
    "\n",
    "# Process each string separately\n",
    "for string in string_list:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', string)\n",
    "    synonyms = generate_synonyms(tokens, num_synonyms, full_return=True)\n",
    "    synonym_list.extend(synonyms + tokens)\n",
    "    \n",
    "target_topics[\"bookeeping\"] = synonym_list    \n",
    "\n",
    "# INTERACTION\n",
    "\n",
    "# Actions\n",
    "string1 = \"solo, interaction, interact, roleplay, discuss, talk, influence, defeat, lose, win, work together, debate, act\"\n",
    "# Components\n",
    "string2 = \"solo, interaction, interactivity, team, group, friends, family, wife, number, npc, role, participants, actions, solo, coop, action, turn\"\n",
    "# Negative connotations\n",
    "string3 = \"solo, interaction, conflict, discuss, clash, face, defeat, lose, fight, argue, angry, bored\"\n",
    "# Positive connotations\n",
    "string4 = \"solo, interaction, cooperation, teamwork, interactive, engaging, friendly, bonding, interesting\"\n",
    "# Other related terms\n",
    "string5 = \"interaction, solo, more, less, players, more people, more players, more gamers\"\n",
    "\n",
    "string_list = [string1, string2, string3, string4, string5]\n",
    "synonym_list = []\n",
    "\n",
    "# Process each string separately\n",
    "for string in string_list:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', string)\n",
    "    synonyms = generate_synonyms(tokens, num_synonyms, full_return=True)\n",
    "    synonym_list.extend(synonyms + tokens)\n",
    "    \n",
    "target_topics[\"interaction\"] = synonym_list\n",
    "\n",
    "# COMPLEX\n",
    "\n",
    "# Actions\n",
    "string1 = \"master, learn, teach, solve, help, understand, replay, enjoy, begin, start, introduce, simplify, improve, predict\"\n",
    "# Components\n",
    "string2 = \"tricks, variables, ability, skill, challenge, depth, strategy, tactics, problems , puzzles, consecuences, repercussions, replayability, modularity, complexity\"\n",
    "# Negative connotations\n",
    "string3 = \"difficult, challenging, simple, complex, hard, demanding, hardcore\"\n",
    "# Positive connotations\n",
    "string4 = \"experience, easy, complex, simple, excellent, unpredictable, replayable, helpful, deep, rich, style,  acessible, competitive\"\n",
    "# Other related terms\n",
    "string5 = \"veteran, nobie, rookie, learning curve, skill level\"\n",
    "\n",
    "string_list = [string1, string2, string3, string4, string5]\n",
    "synonym_list = []\n",
    "\n",
    "# Process each string separately\n",
    "for string in string_list:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', string)\n",
    "    synonyms = generate_synonyms(tokens, num_synonyms, full_return=True)\n",
    "    synonym_list.extend(synonyms + tokens)\n",
    "\n",
    "target_topics[\"complex\"] = synonym_list\n",
    "\n",
    "# COMPLICATED\n",
    "\n",
    "# Actions\n",
    "string1 = \"complicate, repeat, learn, teach, explain, react, forget, forgive\"\n",
    "# Components\n",
    "string2 = \"many, complication, rules, exeptions, reaction, time, predictable results\"\n",
    "# Negative connotations\n",
    "string3 = \"predictable, hard, easy, boring, daunting, overwhelming, long, endless, repetitive, convoluted\"\n",
    "# Positive connotations\n",
    "string4 = \"easy, quick, forgiving, predictable\"\n",
    "# Other related terms\n",
    "string5 = \"casual, begginers, noobs, no, negative, ease, clutter\"\n",
    "\n",
    "string_list = [string1, string2, string3, string4, string5]\n",
    "synonym_list = []\n",
    "\n",
    "# Process each string separately\n",
    "for string in string_list:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', string)\n",
    "    synonyms = generate_synonyms(tokens, num_synonyms, full_return=True)\n",
    "    synonym_list.extend(synonyms + tokens)\n",
    "\n",
    "target_topics[\"complicated\"] = synonym_list\n",
    "\n",
    "# DOWNTIME\n",
    "\n",
    "# Actions\n",
    "string1 = \"time, relax, think, plan ahead, wait, waste time, do, choose, decide, interact, bore, hope, speed\"\n",
    "# Components\n",
    "string2 = \"time, downtime, free time, waiting period, turns, something, nothing, interactive, in-between, between\"\n",
    "# Negative connotations\n",
    "string3 = \"time, unproductive, long, slow, boring, uninteresting, limit\"\n",
    "# Positive connotations\n",
    "string4 = \"time, fast, quick, engaging, pace, decisions, options, limitless\"\n",
    "# Other related terms\n",
    "string5 = \"time, in-character, individual\"\n",
    "\n",
    "string_list = [string1, string2, string3, string4, string5]\n",
    "synonym_list = []\n",
    "\n",
    "# Process each string separately\n",
    "for string in string_list:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', string)\n",
    "    synonyms = generate_synonyms(tokens, num_synonyms, full_return=True)\n",
    "    synonym_list.extend(synonyms + tokens)\n",
    "\n",
    "target_topics[\"downtime\"] = synonym_list\n",
    "\n",
    "# BASH THE LEADER\n",
    "\n",
    "# Actions\n",
    "string1 = \"catch up, finish, bash, dethrone, lead, rule, fix, win, defeat, overtake, resign, benefit, sacrifice, prevent victory, fight, unite, end, against, curb\"\n",
    "# Components\n",
    "string2 = \"end, endgame, leader, champion, winner, looser, rest, power, gap, difference, advantage, victory, defeat, opprtunity, actions\"\n",
    "# Negative connotations\n",
    "string3 = \"deterministic, fixed, decided, univitable, detriment, disadvatage, losers, meaningless, useless\"\n",
    "# Positive connotations\n",
    "string4 = \"possible, hope, underdog, suprise, turntable, martyr, decision, revolution, winners\"\n",
    "# Other related terms\n",
    "string5 = \"first, second, third, last\"\n",
    "\n",
    "string_list = [string1, string2, string3, string4, string5]\n",
    "synonym_list = []\n",
    "\n",
    "# Process each string separately\n",
    "for string in string_list:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', string)\n",
    "    synonyms = generate_synonyms(tokens, num_synonyms, full_return=True)\n",
    "    synonym_list.extend(synonyms + tokens)\n",
    "    \n",
    "target_topics[\"bash the leader\"] = synonym_list\n",
    "\n",
    "# LUCK\n",
    "\n",
    "# Actions\n",
    "string1 = \"bet, roll dice, gambler, predict, try, intervice, influence, control, result, draw cards\"\n",
    "# Components\n",
    "string2 = \"luck, alea, randomess, uncertainty, unpredictability, outcome, possibilities, opprtunities, probability, chances, intervention\"\n",
    "# Negative connotations\n",
    "string3 = \"unlucky, random, uncontrollable, unpredictable, impossible, jynx, unwinnable, unprobable, unlikely\"\n",
    "# Positive connotations\n",
    "string4 = \"lucky, random, possible, predictable, controlable, winnable, probable, likely\"\n",
    "# Other related terms\n",
    "string5 = \"chances, win, lose, dice, card, deck, skill, control, mastery, practice, expert\"\n",
    "\n",
    "string_list = [string1, string2, string3, string4, string5]\n",
    "synonym_list = []\n",
    "\n",
    "# Process each string separately\n",
    "for string in string_list:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', string)\n",
    "    synonyms = generate_synonyms(tokens, num_synonyms, full_return=True)\n",
    "    synonym_list.extend(synonyms + tokens)\n",
    "\n",
    "target_topics[\"luck\"] = synonym_list\n",
    "\n",
    "# Contraries of every topic\n",
    "\n",
    "string1 = \"\"\n",
    "string2 = \"lawless, chaotic, slow, math, mathematics\"\n",
    "string3 = \"poor\"\n",
    "\n",
    "# Actions\n",
    "string1 = \"\"\n",
    "# Components\n",
    "string2 = \"performance, reviews\"\n",
    "# Negative connotations\n",
    "string3 = \"lawless, poor, chaotic, negative, dislike\"\n",
    "# Positive connotations\n",
    "string4 = \"lawful, rich, ordered, possitive, like\"\n",
    "# Other related terms\n",
    "string5 = \"\"\n",
    "\n",
    "string_list = [string1, string2, string3, string4, string5]\n",
    "synonym_list = []\n",
    "\n",
    "# Process each string separately\n",
    "for string in string_list:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', string)\n",
    "    synonyms = generate_synonyms(tokens, num_synonyms, full_return=True)\n",
    "    synonym_list.extend(synonyms + tokens)\n",
    "\n",
    "target_topics[\"complements\"] = synonym_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b61f81",
   "metadata": {},
   "source": [
    "# COMPLEMENTING WITH CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(400)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize and stem text\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    return [lemmatize_stemming(token) for token in simple_preprocess(text) if token not in STOPWORDS and len(token) > 2]\n",
    "\n",
    "# Create merged_list with replaced underscores\n",
    "merged_list = [item.replace(\"_\", \" \") for sublist in target_topics.values() for item in sublist]\n",
    "\n",
    "# Initialize variables\n",
    "processed_docs = []\n",
    "reference_sheet = {}\n",
    "topic_reference_sheet = {}\n",
    "\n",
    "# Create new_list by splitting strings with multiple words\n",
    "new_list = [word for string in merged_list for word in string.split() if len(string.split()) > 1]\n",
    "\n",
    "# print('*', len(merged_list), len(new_list))        \n",
    "\n",
    "# Process each document\n",
    "for doc in merged_list:\n",
    "    processed_words = preprocess(doc)\n",
    "    processed_docs.append(processed_words)\n",
    "    \n",
    "    # Create reference_sheet and topic_reference_sheet\n",
    "    for word in processed_words:\n",
    "        reference_sheet.setdefault(word, []).append(doc)\n",
    "        for key, values in target_topics.items():\n",
    "            if doc in values:\n",
    "                topic_reference_sheet.setdefault(word, []).append(key)\n",
    "\n",
    "# Simplify the creation of reference_sheet and topic_reference_sheet\n",
    "reference_sheet = {key: values for key, values in reference_sheet.items()}     \n",
    "topic_reference_sheet = {key: values for key, values in topic_reference_sheet.items()}   \n",
    "\n",
    "# print(len(processed_docs))\n",
    "# print(len(reference_sheet))\n",
    "# print(len(topic_reference_sheet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d8971",
   "metadata": {},
   "source": [
    "# TRIMMING DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read words from a file and return them as a list\n",
    "def read_words_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        word_list = [line.strip() for line in file]\n",
    "    return word_list\n",
    "\n",
    "print('Dictionary from corpus length:', len(dictionary))\n",
    "topic_dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print('Dictionary from code length:', len(topic_dictionary), '\\n')\n",
    "\n",
    "# Merge the dictionaries into a single dictionary\n",
    "merged_dict = gensim.corpora.Dictionary()\n",
    "merged_dict.merge_with(dictionary)\n",
    "merged_dict.merge_with(topic_dictionary)\n",
    "\n",
    "filename = 'words_to_remove.txt'\n",
    "words_to_remove = read_words_from_file(filename)\n",
    "\n",
    "print(words_to_remove, '\\n')\n",
    "print('Merged dictionary length:', len(merged_dict))\n",
    "\n",
    "# Filter and compactify the merged dictionary\n",
    "merged_dict.filter_tokens(lambda token_id, token_count: merged_dict[token_id] not in words_to_remove)\n",
    "merged_dict.compactify()\n",
    "\n",
    "print('Trimmed dictionary length:', len(merged_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb48b4",
   "metadata": {},
   "source": [
    "# CREATING DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged dictionary to a file\n",
    "merged_dict.save('dictionary')\n",
    "\n",
    "import json\n",
    "\n",
    "# Convert the reference_sheet dictionary to JSON format\n",
    "json_data = json.dumps(reference_sheet)\n",
    "\n",
    "# Write the JSON data to reference_sheet.json file\n",
    "with open('reference_sheet.json', 'w') as file:\n",
    "    file.write(json_data)\n",
    "\n",
    "# Convert the topic_reference_sheet dictionary to JSON format\n",
    "json_data = json.dumps(topic_reference_sheet)\n",
    "\n",
    "# Write the JSON data to topic_reference_sheet.json file\n",
    "with open('topic_reference_sheet.json', 'w') as file:\n",
    "    file.write(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a9770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
