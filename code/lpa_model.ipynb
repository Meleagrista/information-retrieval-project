{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf17d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e4d00",
   "metadata": {},
   "source": [
    "# READ DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb5bfb4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length: 1925 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Load the dictionary from file\n",
    "dictionary = Dictionary.load('dictionary')\n",
    "print('Dictionary length:', len(dictionary), '\\n')\n",
    "\n",
    "import json\n",
    "\n",
    "# Load data from reference_sheet.json\n",
    "with open('reference_sheet.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "reference_sheet = json.loads(json_data)\n",
    "\n",
    "# Load data from topic_reference_sheet.json\n",
    "with open('topic_reference_sheet.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "topic_reference_sheet = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3e528",
   "metadata": {},
   "source": [
    "# READ CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bd16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the corpus: 40175 \n",
      "\n",
      "Word 14 ('charact') appears 1 time.\n",
      "Word 40 ('interest') appears 1 time.\n",
      "Word 43 ('lot') appears 1 time.\n",
      "Word 201 ('situat') appears 1 time.\n",
      "Word 1016 ('privat') appears 1 time.\n",
      "Word 1810 ('adventur') appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "if demo:\n",
    "    # Set the path for the original data directory\n",
    "    path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR\\files_csv\\version_demo'\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(path_original_data, 'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "else:\n",
    "    path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR\\files_csv'\n",
    "    df = pd.read_csv(os.path.join(path_original_data, 'post_processed_comment_data.csv'), low_memory=False)\n",
    "\n",
    "# Extract the 'gensim_comment' column as a list of strings\n",
    "string_list = df['gensim_comment'].tolist()\n",
    "\n",
    "# Convert the string representation of lists to actual lists\n",
    "comments_list = [ast.literal_eval(s) for s in string_list]\n",
    "\n",
    "# Convert the comments to bag-of-words representation using the loaded dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in comments_list]\n",
    "print('Length of the corpus:', len(bow_corpus), '\\n')\n",
    "\n",
    "# Choose a random document from the corpus\n",
    "random_number = round(random.uniform(0, len(bow_corpus)))\n",
    "bow_doc_x = bow_corpus[random_number]\n",
    "\n",
    "# Print the word count for each word in the chosen document\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} ('{}') appears {} time.\".format(bow_doc_x[i][0], dictionary[bow_doc_x[i][0]], bow_doc_x[i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dab6ee",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4d4b8",
   "metadata": {},
   "source": [
    "### IMPORTANT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f27dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF TOPICS\n",
    "topic_num = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478438f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import models\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create an instance of LdaMulticore model\n",
    "lda = gensim.models.LdaMulticore(\n",
    "    corpus=bow_corpus,    # The bag-of-words corpus\n",
    "    num_topics=topic_num, # Number of topics to generate\n",
    "    id2word=dictionary,   # Mapping of word IDs to words\n",
    "    passes=50,            # Number of passes through the corpus\n",
    "    workers=2             # Number of worker processes\n",
    ")\n",
    "\n",
    "# Alternative approach using LdaModel\n",
    "# lda = gensim.models.LdaModel(\n",
    "#     corpus=bow_corpus,\n",
    "#     num_topics=topic_num,\n",
    "#     id2word=dictionary,\n",
    "#     passes=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3446994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim import models\n",
    "\n",
    "if demo:\n",
    "    # Set the path for the original data directory\n",
    "    path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR\\files_lpa_model\\version_demo_final'\n",
    "    print('Using demo...')\n",
    "else:\n",
    "    path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR\\files_lpa_model'\n",
    "\n",
    "# Create the path for the temporary file\n",
    "temp_file = os.path.join(path_original_data, 'lda_model')\n",
    "\n",
    "# Save the LdaModel to the temporary file\n",
    "# lda.save(temp_file)\n",
    "\n",
    "# Load the LdaModel from the temporary file\n",
    "lda = models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079657c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Still unread: [2, 3, 5, 9, 12, 20, 21, 22, 23, 24]\n",
      "- Still unread: [2, 22]\n",
      "- Still unread: []\n"
     ]
    }
   ],
   "source": [
    "topics_read = list(range(0, topic_num))\n",
    "topic_dict = {}\n",
    "\n",
    "# Loop until all topics have been read\n",
    "while len(topics_read) > 0:\n",
    "    # Iterate over the topics\n",
    "    for topic in lda.print_topics():\n",
    "        topic_id, topic_words = topic\n",
    "        # Check if the current topic is in the unread topics list\n",
    "        if topic_id in topics_read:\n",
    "            topics_read.remove(topic_id)\n",
    "            words = topic_words.split(' + ')\n",
    "            topic_dict[topic_id] = {}\n",
    "            # Process each word in the topic\n",
    "            for i, word in enumerate(words):\n",
    "                word = word.split('\"')\n",
    "                # Check if the word is in the reference_sheet\n",
    "                if word[1] in reference_sheet:\n",
    "                    topic_dict[topic_id][word[1]] = (word[0].replace('*', ''), reference_sheet[word[1]])\n",
    "                else:\n",
    "                    topic_dict[topic_id][word[1]] = (word[0].replace('*', ''), [word[1].upper()])\n",
    "    print('- Still unread:', topics_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8f69e",
   "metadata": {},
   "source": [
    "# RESULTING TOPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98979d7",
   "metadata": {},
   "source": [
    "### IMPORTANT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "365c9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOLEAN - True FOR READBLE WORDS False FOR WORD OF ORIGIN\n",
    "readable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c1262f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC: 0\n",
      "interest : interesting\n",
      "decis : decision\n",
      "quick : quick\n",
      "choic : choice\n",
      "feel : feel\n",
      "engag : engage\n",
      "right : rightful\n",
      "turn : turn\n",
      "gameplay : -\n",
      "simpl : simple\n",
      "\n",
      "\n",
      "TOPIC: 1\n",
      "better : better\n",
      "need : need\n",
      "copi : copy\n",
      "definit : -\n",
      "repetit : repetition\n",
      "coop : coop\n",
      "complic : complicate\n",
      "rank : rank\n",
      "categori : category\n",
      "materi : material\n",
      "\n",
      "\n",
      "TOPIC: 2\n",
      "stori : story\n",
      "world : world\n",
      "adventur : adventure\n",
      "charact : character\n",
      "campaign : campaign\n",
      "tell : tell\n",
      "experi : experience\n",
      "combat : combat\n",
      "cooper : cooperation\n",
      "feel : feel\n",
      "\n",
      "\n",
      "TOPIC: 3\n",
      "action : action\n",
      "turn : turn\n",
      "board : board\n",
      "round : round\n",
      "end : end\n",
      "point : point\n",
      "way : way\n",
      "order : order\n",
      "track : track\n",
      "differ : difference\n",
      "\n",
      "\n",
      "TOPIC: 4\n",
      "worker : -\n",
      "placement : -\n",
      "resourc : -\n",
      "manag : manage\n",
      "build : build\n",
      "combin : combine\n",
      "season : seasoned\n",
      "engin : engineer\n",
      "free : free\n",
      "art : -\n",
      "\n",
      "\n",
      "TOPIC: 5\n",
      "thing : -\n",
      "slow : slow\n",
      "hear : hear\n",
      "genius : genius\n",
      "memori : memory\n",
      "demand : demand\n",
      "multitud : multitude\n",
      "coher : coherent\n",
      "happen : happen\n",
      "charg : charge\n",
      "\n",
      "\n",
      "TOPIC: 6\n",
      "rat : -\n",
      "base : base\n",
      "add : add\n",
      "race : race\n",
      "improv : improve\n",
      "initi : initiate\n",
      "opinion : opinion\n",
      "rate : rate\n",
      "chang : change\n",
      "slight : slight\n",
      "\n",
      "\n",
      "TOPIC: 7\n",
      "lot : lot\n",
      "high : -\n",
      "replay : replay\n",
      "interact : interact\n",
      "strategi : strategy\n",
      "low : low\n",
      "abil : ability\n",
      "mix : mix\n",
      "complex : complex\n",
      "head : head\n",
      "\n",
      "\n",
      "TOPIC: 8\n",
      "compon : -\n",
      "art : -\n",
      "amaz : amaze\n",
      "gameplay : -\n",
      "qualiti : quality\n",
      "artwork : -\n",
      "product : product\n",
      "fantast : -\n",
      "excel : excellent\n",
      "board : board\n",
      "\n",
      "\n",
      "TOPIC: 9\n",
      "pack : pack\n",
      "board : board\n",
      "combat : combat\n",
      "encount : encounter\n",
      "fight : fight\n",
      "resourc : -\n",
      "wind : wind\n",
      "extens : extension\n",
      "item : item-by-item\n",
      "machin : machinate\n",
      "\n",
      "\n",
      "TOPIC: 10\n",
      "scenario : -\n",
      "campaign : campaign\n",
      "origin : original\n",
      "start : start\n",
      "charact : character\n",
      "book : book\n",
      "set : set\n",
      "finish : finish\n",
      "new : -\n",
      "level : level\n",
      "\n",
      "\n",
      "TOPIC: 11\n",
      "feel : feel\n",
      "mean : mean\n",
      "know : know\n",
      "think : think\n",
      "need : need\n",
      "end : end\n",
      "leav : leave\n",
      "away : away\n",
      "design : design\n",
      "way : way\n",
      "\n",
      "\n",
      "TOPIC: 12\n",
      "peopl : people\n",
      "group : group\n",
      "friend : friend\n",
      "famili : family\n",
      "gamer : gamers\n",
      "board : board\n",
      "new : -\n",
      "expect : expect\n",
      "casual : casual\n",
      "minut : minute\n",
      "\n",
      "\n",
      "TOPIC: 13\n",
      "tile : -\n",
      "score : -\n",
      "abstract : -\n",
      "point : point\n",
      "place : place\n",
      "lay : lay\n",
      "simpl : simple\n",
      "old : old\n",
      "board : board\n",
      "puzzl : puzzle\n",
      "\n",
      "\n",
      "TOPIC: 14\n",
      "rule : rule\n",
      "understand : understand\n",
      "read : read\n",
      "lot : lot\n",
      "rulebook : rulebook\n",
      "learn : learn\n",
      "clear : clear\n",
      "difficult : difficult\n",
      "book : book\n",
      "complex : complex\n",
      "\n",
      "\n",
      "TOPIC: 15\n",
      "long : long\n",
      "take : taking\n",
      "hour : -\n",
      "setup : setup\n",
      "total : total\n",
      "battl : battle\n",
      "bite : bit\n",
      "set : set\n",
      "tabl : -\n",
      "lord : lord\n",
      "\n",
      "\n",
      "TOPIC: 16\n",
      "love : love\n",
      "favorit : -\n",
      "wife : wife\n",
      "far : -\n",
      "right : rightful\n",
      "definit : -\n",
      "tri : try\n",
      "wait : wait\n",
      "moment : moment\n",
      "probabl : probable\n",
      "\n",
      "\n",
      "TOPIC: 17\n",
      "box : -\n",
      "edit : edition\n",
      "big : big\n",
      "complet : complete\n",
      "insert : insert\n",
      "come : come\n",
      "wait : wait\n",
      "organ : organize\n",
      "open : open\n",
      "break : break\n",
      "\n",
      "\n",
      "TOPIC: 18\n",
      "nice : -\n",
      "bite : bit\n",
      "littl : little\n",
      "feel : feel\n",
      "puzzl : puzzle\n",
      "version : version\n",
      "light : light\n",
      "piec : piece\n",
      "bore : bore\n",
      "prefer : prefer\n",
      "\n",
      "\n",
      "TOPIC: 19\n",
      "solo : solo\n",
      "best : -\n",
      "far : -\n",
      "mode : mode\n",
      "experi : experience\n",
      "probabl : probable\n",
      "tri : try\n",
      "review : review\n",
      "excel : excellent\n",
      "count : count\n",
      "\n",
      "\n",
      "TOPIC: 20\n",
      "easi : easy\n",
      "learn : learn\n",
      "teach : teach\n",
      "quick : quick\n",
      "master : master\n",
      "pick : pick\n",
      "hard : hard\n",
      "simpl : simple\n",
      "curv : curve\n",
      "fast : fast\n",
      "\n",
      "\n",
      "TOPIC: 21\n",
      "think : think\n",
      "pretti : -\n",
      "go : going\n",
      "lot : lot\n",
      "sure : -\n",
      "bite : bit\n",
      "fast : fast\n",
      "plan : plan\n",
      "wish : wish\n",
      "feel : feel\n",
      "\n",
      "\n",
      "TOPIC: 22\n",
      "dice : dice\n",
      "luck : luck\n",
      "random : random\n",
      "draw : draw\n",
      "bad : -\n",
      "roll : roll\n",
      "skill : skill\n",
      "pattern : pattern\n",
      "frustrat : frustration\n",
      "use : use\n",
      "\n",
      "\n",
      "TOPIC: 23\n",
      "look : look\n",
      "tabl : -\n",
      "set : set\n",
      "forward : forward\n",
      "hard : hard\n",
      "core : hard-core\n",
      "releas : release\n",
      "tri : try\n",
      "hook : hook\n",
      "talk : talk\n",
      "\n",
      "\n",
      "TOPIC: 24\n",
      "star : star\n",
      "doctor : doctor\n",
      "luck : luck\n",
      "rat : -\n",
      "replay : replay\n",
      "qualiti : quality\n",
      "compon : -\n",
      "rule : rule\n",
      "low : low\n",
      "interact : interact\n",
      "\n",
      "\n",
      "TOPIC: 25\n",
      "deck : deck\n",
      "build : build\n",
      "set : set\n",
      "limit : limit\n",
      "power : power\n",
      "variabl : variable\n",
      "base : base\n",
      "goal : goal\n",
      "core : hard-core\n",
      "modular : modularity\n",
      "\n",
      "\n",
      "TOPIC: 26\n",
      "faction : -\n",
      "win : win\n",
      "differ : difference\n",
      "balanc : balance\n",
      "victori : victory\n",
      "way : way\n",
      "strategi : strategy\n",
      "point : point\n",
      "board : board\n",
      "end : end\n",
      "\n",
      "\n",
      "TOPIC: 27\n",
      "euro : -\n",
      "engin : engineer\n",
      "build : build\n",
      "control : control\n",
      "style : style\n",
      "combat : combat\n",
      "heavi : heavy\n",
      "interact : interact\n",
      "conflict : conflict\n",
      "direct : direct\n",
      "\n",
      "\n",
      "TOPIC: 28\n",
      "strategi : strategy\n",
      "design : design\n",
      "perfect : -\n",
      "complex : complex\n",
      "simpl : simple\n",
      "depth : depth\n",
      "deep : deep\n",
      "tactic : tactic\n",
      "gameplay : -\n",
      "balanc : balance\n",
      "\n",
      "\n",
      "TOPIC: 29\n",
      "differ : difference\n",
      "new : -\n",
      "charact : character\n",
      "hero : hero\n",
      "challeng : challenge\n",
      "feel : feel\n",
      "champion : champion\n",
      "bring : bring\n",
      "aspect : aspect\n",
      "way : way\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def find_closest_match(input_string, string_list):\n",
    "    highest_ratio = 0\n",
    "    closest_string = None\n",
    "\n",
    "    # Iterate over the string list\n",
    "    for string in string_list:\n",
    "        # Calculate the fuzz ratio between the input string and the current string\n",
    "        ratio = fuzz.ratio(input_string, string)\n",
    "        # Update the highest ratio and closest string if the current ratio is higher\n",
    "        if ratio > highest_ratio:\n",
    "            highest_ratio = ratio\n",
    "            closest_string = string\n",
    "\n",
    "    return closest_string\n",
    "\n",
    "# Sort the topic_dict by topic key in ascending order\n",
    "ordered_dict = dict(sorted(topic_dict.items(), key=lambda x: int(x[0])))\n",
    "\n",
    "topic_assignment = {}\n",
    "\n",
    "# Iterate over the ordered_dict\n",
    "for topic_key, topic_value in ordered_dict.items():\n",
    "    print('TOPIC:', topic_key)\n",
    "    \n",
    "    # Initialize the topic assignment dictionary for the current topic\n",
    "    topic_assignment[topic_key] = {}\n",
    "    \n",
    "    # Iterate over the words and probabilities in the current topic\n",
    "    for key, value in topic_value.items():\n",
    "        wprob, wlist = value\n",
    "        \n",
    "        word_list = []\n",
    "        \n",
    "        # FIRST PART\n",
    "        \n",
    "        # Split each string in wlist and add the words to word_list\n",
    "        for string in wlist:\n",
    "            words = string.split()\n",
    "            word_list.extend(words)\n",
    "        \n",
    "        # Find the closest match for the key in the word_list\n",
    "        closest_match = find_closest_match(key, word_list)\n",
    "        \n",
    "        # If no closest match is found, set it to the shortest string in wlist with an asterisk\n",
    "        if closest_match is None:\n",
    "            closest_match = min(wlist, key=len) + '*'\n",
    "        \n",
    "        # If the closest match is in uppercase, set it to '-'\n",
    "        if closest_match.isupper():\n",
    "            closest_match = '-'\n",
    "        \n",
    "        # Print the word and the closest match\n",
    "        if readable: print(key, ':', closest_match.lower())\n",
    "        \n",
    "        # SECOND PART\n",
    "        \n",
    "        # Check if the word is in the topic_reference_sheet\n",
    "        if key in topic_reference_sheet:\n",
    "            # Assign the corresponding value to topic_origin in topic_assignment\n",
    "            temp_list = list(set(topic_reference_sheet[key]))\n",
    "            \n",
    "            topic_assignment[topic_key][key] = temp_list\n",
    "            \n",
    "            # If there are multiple topic origins, join them with commas\n",
    "            if len(temp_list) > 1:\n",
    "                topic_origin = ', '.join(temp_list)\n",
    "            else:\n",
    "                topic_origin = temp_list[0]\n",
    "        else:\n",
    "            # If the word is not in the topic_reference_sheet, set topic_origin to '-'\n",
    "            topic_origin = '-'\n",
    "            \n",
    "            # Store the topic_origin in topic_assignment\n",
    "            topic_assignment[topic_key][key] = topic_origin\n",
    "        \n",
    "        # Print the word and its corresponding topic origin\n",
    "        if not readable: print(key, ':', topic_origin)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45860d78",
   "metadata": {},
   "source": [
    "# LABEL TOPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2141fb4",
   "metadata": {},
   "source": [
    "#### WARNING\n",
    "The names written may not be always representive of the topics at the current moment since even without major changes in content the order they are shown can change.\n",
    "\n",
    "Check before using the manually assigned names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9408aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0: \"Engaging gameplay\", 1: \"Version\", 2: \"Worldbuilding\", 3: \"Turn-based\", 4: \"Some genres\", 5: \"NA\",\n",
    "              6: \"Opinion of version\", 7: \"Replayability\", 8: \"Product quality\", 9: \"Board combat\", 10: \"Campaign setting\",\n",
    "              11: \"Some opinion\", 12: \"Friends and families\", 13: \"Other genres\", 14: \"Rules complexity\", 15: \"Duration\", \n",
    "              16: \"Personal recommendation\",\n",
    "              17: \"Box and components\", 18: \"Lite version\", 19: \"Solo\", 20: \"Learning curve\", 21: \"Other opinions\",\n",
    "              22: \"Luck\", 23: \"NA\", 24: \"Mixture of qualities\", 25: \"Card-based\", 26: \"Winning stategy\", 27: \"Tactical style\",\n",
    "              28: \"COmplexity\", 29: \"Inmersive character\"}\n",
    "\n",
    "my_topics = {\"bookeeping\": [14,8], \"downtime\": [0,3,9], \"interaction\": [12,19,25], \"bash the leader\": [26,27],\n",
    "             \"complex or complicated\": [7,20,25,27,28], \"luck\": [22,25]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276c12a",
   "metadata": {},
   "source": [
    "### IMPORTANT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64bfabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF TIMES A CATEGORY HAS TO APPEAR IN A TOPIC TO BE CONSIDERED AS THE TOPIC\n",
    "topic_bound = 4\n",
    "\n",
    "# THE MAX DIFFERENCE BETWEEN TO ELEMENTS TO CONSIDERED JOINING THEM\n",
    "# If the difference is too big that means one of the categories can stand on its own, if it's less then combining may boost its probabilities.\n",
    "union_bound = 1\n",
    "\n",
    "# BOOLEAN - FUSE CATEGORIES 'COMPLEX' AND 'COMPLICATED'\n",
    "fuse = True\n",
    "\n",
    "# BOOLEAN - FUSE CATEGORIES 'INTERACTION' AND 'DOWNTIME'\n",
    "boost = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b27a813d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ['downtime']\n",
      "1 - ['complex or complicated']\n",
      "2 - ['interaction']\n",
      "3 - ['interaction', 'bash the leader']\n",
      "4 - ['NA']\n",
      "5 - ['complex or complicated']\n",
      "6 - ['NA']\n",
      "7 - ['complex or complicated']\n",
      "8 - ['NA']\n",
      "9 - ['NA']\n",
      "10 - ['bash the leader']\n",
      "11 - ['complex or complicated']\n",
      "12 - ['interaction']\n",
      "13 - ['bookeeping', 'complex or complicated']\n",
      "14 - ['complex or complicated']\n",
      "15 - ['NA']\n",
      "16 - ['NA']\n",
      "17 - ['NA']\n",
      "18 - ['bookeeping', 'complex or complicated']\n",
      "19 - ['NA']\n",
      "20 - ['complex or complicated']\n",
      "21 - ['downtime']\n",
      "22 - ['luck']\n",
      "23 - ['downtime']\n",
      "24 - ['bash the leader']\n",
      "25 - ['bash the leader', 'complex or complicated']\n",
      "26 - ['bash the leader']\n",
      "27 - ['bookeeping', 'bash the leader', 'complex or complicated']\n",
      "28 - ['complex or complicated']\n",
      "29 - ['interaction']\n"
     ]
    }
   ],
   "source": [
    "def return_draw(my_dict):\n",
    "    # Retrieve the first and second elements from the dictionary\n",
    "    first_element = list(my_dict.keys())[0]\n",
    "    second_element = list(my_dict.keys())[1]\n",
    "    # Retrieve the values corresponding to the first and second elements\n",
    "    first_value = my_dict[first_element]\n",
    "    second_value = my_dict[second_element]\n",
    "    \n",
    "    # Check if the values are equal\n",
    "    if first_value == second_value:\n",
    "        # If equal, find all keys with the same value\n",
    "        matching_keys = [key for key, value in my_dict.items() if value == first_value]\n",
    "    else:\n",
    "        # If not equal, consider only the first element\n",
    "        matching_keys = [first_element]\n",
    "\n",
    "    return matching_keys\n",
    "\n",
    "topic_estimation = {}\n",
    "topic_classification = {}\n",
    "\n",
    "# Iterate over the topic_assignment dictionary\n",
    "for topic_key, topic_value in topic_assignment.items():\n",
    "    topic_count = {}\n",
    "    # Iterate over the values in the topic_value dictionary\n",
    "    for key, value in topic_value.items():\n",
    "        # Iterate over each topic in the value list\n",
    "        for topic in value:\n",
    "            # Increment the count for each topic in the topic_count dictionary\n",
    "            current = topic_count.setdefault(topic, 0)\n",
    "            topic_count[topic] = current + 1\n",
    "    \n",
    "    # Sort the topic_count dictionary by count in descending order\n",
    "    sorted_dict = dict(sorted(topic_count.items(), key=lambda x: x[1], reverse=True))\n",
    "    # Remove the '-' key from the sorted_dict if present\n",
    "    if '-' in sorted_dict:\n",
    "        del sorted_dict['-']\n",
    "    \n",
    "    # Assign the sorted_dict to the corresponding topic key in topic_estimation\n",
    "    topic_estimation[topic_key] = sorted_dict        \n",
    "\n",
    "# Iterate over the topic_estimation dictionary\n",
    "for key, value in topic_estimation.items():\n",
    "    if fuse:\n",
    "        key_to_fuse1 = 'complex'\n",
    "        key_to_fuse2 = 'complicated'\n",
    "        # Retrieve the values for key_to_fuse1 and key_to_fuse2 from the value dictionary\n",
    "        value1 = value.setdefault(key_to_fuse1, 0)\n",
    "        value2 = value.setdefault(key_to_fuse2, 0)\n",
    "        # Calculate the sum of the values\n",
    "        sum_value = value1 + value2\n",
    "        # Update the value dictionary with the fused key and sum value\n",
    "        value[\"complex or complicated\"] = sum_value\n",
    "        # Remove the individual keys from the value dictionary\n",
    "        del value[key_to_fuse1]\n",
    "        del value[key_to_fuse2]\n",
    "    \n",
    "    # Additional fusion example\n",
    "    if boost:\n",
    "        key_to_fuse1 = 'downtime'\n",
    "        key_to_fuse2 = 'interaction'\n",
    "        value1 = value.setdefault(key_to_fuse1, 0)\n",
    "        value2 = value.setdefault(key_to_fuse2, 0)\n",
    "        if abs(value1 - value2) <= union_bound:\n",
    "            sum_value = value1 + value2\n",
    "            value[\"game dynamics\"] = sum_value\n",
    "            # del value[key_to_fuse1]\n",
    "            # del value[key_to_fuse2]\n",
    "    \n",
    "    # Sort the value dictionary by count in descending order\n",
    "    value = dict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
    "    # Update the value in the topic_estimation dictionary\n",
    "    topic_estimation[key] = value\n",
    "\n",
    "# Iterate over the topic_estimation dictionary\n",
    "for key, value in topic_estimation.items():\n",
    "    # Retrieve the first element from the value dictionary\n",
    "    first_element = next(iter(value.items()))\n",
    "    name, count = first_element\n",
    "    if count >= topic_bound:\n",
    "        if len(return_draw(value)) == 1:\n",
    "            # If only one key is returned, assign it as the topic classification\n",
    "            topic_classification[key] = [name]\n",
    "        else:\n",
    "            # Otherwise, assign the returned keys as the topic classification\n",
    "            topic_classification[key] = return_draw(value)\n",
    "    else:\n",
    "        # If the count is below the topic_bound, assign 'NA' as the topic classification\n",
    "        topic_classification[key] = ['NA']\n",
    "        \n",
    "# Print the topic key and its corresponding topic classification\n",
    "for key, value in topic_classification.items():       \n",
    "    print(key, '-', value)    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95195fdb",
   "metadata": {},
   "source": [
    "# Alternative manual option to the automatic labeling\n",
    "topic_classification = {\n",
    "    n: [key for key, value in my_topics.items() if n in value] or ['NA']\n",
    "    for n in range(topic_num)\n",
    "}\n",
    "\n",
    "# Print the topic key and its corresponding topic classification\n",
    "for key, value in topic_classification.items():\n",
    "    print(key, '-', value)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf07925",
   "metadata": {},
   "source": [
    "# EXAMINE COMMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6ec39",
   "metadata": {},
   "source": [
    "### IMPORTANT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3723d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After many iteration the next code is quite convoluded. These are the variables need to change it function.\n",
    "\n",
    "# NUMBER OF MATCHES WITH THE DICTIONARY NEEDED TO TRUST THE RESULT\n",
    "# Extremely short messages are prone to have phony results\n",
    "bound = 5\n",
    "\n",
    "# MINIMUM PROBABILITY NEEDED TO STORE THE TOPIC DETECTED\n",
    "# Some comments barely touch some topics, this helps to avoid long list of topics\n",
    "bound_prob = 0.05\n",
    "\n",
    "# BOOLEAN - USE THE AUTOMATICALLY LABELLED TOPICS\n",
    "# Highest priority, overrides the rest of variables\n",
    "bool_auto = True\n",
    "\n",
    "# BOOLEAN - USE THE MANUALLY ASSIGNED INDIVIDUAL TOPIC LABELS INSTEAD THE GENERAL TOPIC LABELS\n",
    "# Second highest priority, overrides the next variable.\n",
    "bool_title = False\n",
    "\n",
    "# BOOLEAN - GROUP ALL MANUALLY ASIGNED GENERAL TOPIC LABELS SO THERE ARE NO DUPLICATES\n",
    "# Lowest priority, if all is false will use the manual general labels with duplicates.\n",
    "bool_group = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cf6f51b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the element in `comments_list`: 332 \n",
      "\n",
      "Best racing game out there as of 2023.  Rules scale nicely to be accessible for family gamers and die hard gamers. \n",
      "\n",
      "Topics detected:\n",
      "INTERACTION - 40.67%\n",
      "LUCK - 15.48%\n",
      "BOOKEEPING - 15.19%\n"
     ]
    }
   ],
   "source": [
    "# Function to print the comment and return the document bag-of-words representation\n",
    "def print_comment(n):\n",
    "    target_bow = bow_corpus[n]\n",
    "    index = next((i for i, bow_element in enumerate(bow_corpus) if bow_element == target_bow), None)\n",
    "\n",
    "    if index is not None:\n",
    "        print(\"Index of the element in `comments_list`:\", index, '\\n')\n",
    "    else:\n",
    "        print(\"Element not found in `comments_list`.\", '\\n')\n",
    "\n",
    "    document_bow = bow_corpus[index]\n",
    "    document_original = \" \".join([dictionary[id] for id, _ in document_bow])\n",
    "\n",
    "    if len(document_original.split()) <= bound:\n",
    "        print('- WARNING: Amount of data insufficient.', '\\n')\n",
    "\n",
    "    element = comments_list[index]\n",
    "    comment = df.loc[index, 'comment']\n",
    "\n",
    "    print(comment, '\\n')\n",
    "\n",
    "    return document_bow\n",
    "\n",
    "# Function to find keys in a dictionary with a given value\n",
    "def find_keys_with_value(dictionary, number):\n",
    "    return [key for key, values in dictionary.items() if number in values]\n",
    "\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "random_number = round(random.uniform(0, len(bow_corpus)))\n",
    "\n",
    "# Get the topics for a randomly selected document and sort them by probability\n",
    "document_topics = lda.get_document_topics(print_comment(random_number))\n",
    "sorted_topics = sorted(document_topics, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "topics_detected = []\n",
    "high_topics_detected = {}\n",
    "for topic_id, topic_prob in sorted_topics:\n",
    "    percentage = topic_prob * 100\n",
    "    formatted_percentage = \"{:.2f}%\".format(percentage)\n",
    "    if topic_prob >= bound_prob:\n",
    "        if bool_auto:\n",
    "            # If bool_auto is True, accumulate the topic probabilities for each topic\n",
    "            for topic in topic_classification[topic_id]:\n",
    "                high_topics_detected[topic] = high_topics_detected.get(topic, 0) + percentage\n",
    "        else:\n",
    "            if bool_title:\n",
    "                # If bool_title is True, include topic names in the detected topics\n",
    "                if topic_id in topic_names:\n",
    "                    topics_detected.append(f\"[{topic_id}] Topic: {topic_names[topic_id]} - {formatted_percentage}\")\n",
    "                else:\n",
    "                    topics_detected.append(f\"TOPIC {topic_id} - {formatted_percentage}\")\n",
    "            else:\n",
    "                # If bool_title is False, group topics with the same ID and accumulate their probabilities\n",
    "                topics = find_keys_with_value(my_topics, topic_id)\n",
    "                for elem in topics:\n",
    "                    high_topics_detected.setdefault(elem, []).append(percentage)\n",
    "                if len(topics) > 1:\n",
    "                    topics_detected.append(f\"{', '.join(topics).upper()} ({topic_id}) - {formatted_percentage}\")\n",
    "                elif len(topics) == 1:\n",
    "                    topics_detected.append(f\"{topics[0].upper()} ({topic_id}) - {formatted_percentage}\")\n",
    "\n",
    "print(\"Topics detected:\")\n",
    "if bool_auto:\n",
    "    # If bool_auto is True, print the high_topics_detected dictionary sorted by probabilities\n",
    "    high_topics_detected = dict(sorted(high_topics_detected.items(), key=lambda x: x[1], reverse=True))\n",
    "    for key, value in high_topics_detected.items():\n",
    "        if key != 'NA':\n",
    "            p = \"{:.2f}%\".format(value)\n",
    "            print(f\"{key.upper()} - {p}\")\n",
    "else:\n",
    "    sum_dict = {key: sum(lst) for key, lst in high_topics_detected.items()}\n",
    "    if len(topics_detected) == 0:\n",
    "        print('None.')\n",
    "    else:\n",
    "        if bool_group and not bool_title:\n",
    "            # If bool_group is True and bool_title is False, group topics by summing their probabilities\n",
    "            for key, value in sum_dict.items():\n",
    "                p = \"{:.2f}%\".format(value)\n",
    "                print(f\"{key.upper()} - {p}\")\n",
    "        else:\n",
    "            # Print the individual detected topics\n",
    "            for topic in topics_detected:\n",
    "                print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e2fbd1",
   "metadata": {},
   "source": [
    "# SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a51bb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if demo:\n",
    "    # Set the path for the original data directory\n",
    "    path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR\\files_csv\\version_demo_final'\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(path_original_data, 'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "    print('Using demo..')\n",
    "else:\n",
    "    path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR\\files_csv'\n",
    "    df = pd.read_csv(os.path.join(path_original_data, 'post_processed_comment_data.csv'), low_memory=False)\n",
    "\n",
    "# List of topics\n",
    "topic_list = [\"complex or complicated\", \"luck\", \"interaction\", \"bash the leader\", \"downtime\", \"bookeeping\"]\n",
    "\n",
    "# Precompute and store the document topics\n",
    "document_topics = [lda.get_document_topics(bow) for bow in bow_corpus]\n",
    "\n",
    "# Define a function to compute topic estimation\n",
    "def topic_estimation(n, input_topic):\n",
    "    # Calculate the sum of topic probabilities for the given input topic\n",
    "    prob_count = sum(topic_prob for topic_id, topic_prob in document_topics[n] if input_topic in topic_classification[topic_id])\n",
    "    return prob_count\n",
    "\n",
    "# Define a function to compute keyword matches\n",
    "def keyword_matches(n):\n",
    "    # Get the bag-of-words representation for the document\n",
    "    document_bow = bow_corpus[n]\n",
    "    document_bow_ids = [id for id, _ in document_bow]\n",
    "    # Convert the word IDs to their corresponding words in the dictionary\n",
    "    document_original = \" \".join([dictionary[id] for id in document_bow_ids])\n",
    "    # Count the number of keywords (words in the dictionary) in the document\n",
    "    return len(document_original.split())\n",
    "\n",
    "# Add a new column 'dictionary_matches' using vectorized operations\n",
    "df['dictionary_matches'] = df.index.map(keyword_matches)\n",
    "\n",
    "# Create new columns for each topic in topic_list using vectorized operations\n",
    "for topic in topic_list:\n",
    "    # Compute the topic estimation for each document\n",
    "    df[topic.replace(' ', '_') + '_estimation'] = [topic_estimation(n, topic) for n in df.index]\n",
    "\n",
    "# Multiply topic estimations with dictionary matches to get relative values\n",
    "for topic in topic_list:\n",
    "    column_name = topic.replace(' ', '_') + '_relative_value'\n",
    "    df[column_name] = df[topic.replace(' ', '_') + '_estimation'] * df['dictionary_matches']\n",
    "\n",
    "# Rename the topic estimation columns\n",
    "df.rename(columns={topic: topic.replace(' ', '_') + '_estimation' for topic in topic_list}, inplace=True)\n",
    "\n",
    "if demo:\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    df.to_csv(os.path.join(path_original_data, 'lpa_comment_data_demo.csv'), index=False)\n",
    "    print('Using demo...')\n",
    "else:\n",
    "    df.to_csv(os.path.join(path_original_data, 'lpa_comment_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c8788",
   "metadata": {},
   "source": [
    "# TRIM DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed2cf294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feel: 6\n",
      "board: 6\n",
      "simpl: 4\n",
      "way: 4\n",
      "set: 4\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Function to write a list of words to a file\n",
    "def write_words_to_file(word_list, filename):\n",
    "    with open(filename, 'a') as file:\n",
    "        for word in word_list:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "# Get the topic-word distribution matrix\n",
    "topic_word_matrix = lda.get_topics()\n",
    "\n",
    "# Get the vocabulary from the LDA model\n",
    "vocab = lda.id2word\n",
    "\n",
    "# Create a dictionary to store word counts\n",
    "word_counts = {}\n",
    "\n",
    "# Iterate over each topic\n",
    "for topic_idx, topic_words in enumerate(topic_word_matrix):\n",
    "    \n",
    "    # Sort the word indices based on the word probabilities in descending order\n",
    "    word_indices = topic_words.argsort()[::-1]\n",
    "    \n",
    "    # Iterate over the top 10 words for the topic\n",
    "    for rank, word_idx in enumerate(word_indices[:10]):\n",
    "        word = vocab[word_idx]\n",
    "        word_prob = topic_words[word_idx]\n",
    "        \n",
    "        # Increment the count for the word in the word_counts dictionary\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "        \n",
    "        # Print the rank, word, and probability for each word in the topic\n",
    "        # print(f\"   {rank + 1}. {word}: {word_prob:.4f}\")\n",
    "\n",
    "# Sort the word counts in descending order\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Initialize a list to store words that occur more than twice\n",
    "words_to_remove = []\n",
    "\n",
    "# Iterate over the sorted word counts\n",
    "for word, count in sorted_word_counts:\n",
    "    if count > 3:\n",
    "        # Print the word and its count if it occurs more than twice\n",
    "        print(f'{word}: {count}')\n",
    "        words_to_remove.append(word)\n",
    "\n",
    "# Write the words to be removed to a file\n",
    "# write_words_to_file(words_to_remove, 'words_to_remove.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e6572",
   "metadata": {},
   "source": [
    "### ORIGINAL FUNCTION FOR CHECKING RESULTS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5147864c",
   "metadata": {},
   "source": [
    "def extract_comment(n):\n",
    "    target_bow = bow_corpus[n]  # Example target `bow_corpus` element to find its corresponding index in `comments_list`\n",
    "\n",
    "    index = next((i for i, bow_element in enumerate(bow_corpus) if bow_element == target_bow), None)\n",
    "\n",
    "    if index is not None:\n",
    "        print(\"Index of the element in `comments_list`:\", index)\n",
    "    else:\n",
    "        print(\"Element not found in `comments_list`.\")\n",
    "    print()\n",
    "\n",
    "    document_bow = bow_corpus[index]\n",
    "\n",
    "    # Convert the document back to its original form\n",
    "    document_original = \" \".join([dictionary[id] for id, _ in document_bow])\n",
    "\n",
    "    # Print the original document\n",
    "    print(\"Key words in dictionary:\")\n",
    "    print(document_original)\n",
    "    print()\n",
    "\n",
    "    element = comments_list[index]\n",
    "\n",
    "    # Retrieve the corresponding 'comment' field from df\n",
    "    comment = df.loc[index, 'comment']\n",
    "\n",
    "    print(\"Key words from comments_list:\")\n",
    "    print(element)\n",
    "    print()\n",
    "    print(\"Corresponding 'comment' field:\")\n",
    "    print(comment)\n",
    "    print()\n",
    "\n",
    "    return document_bow\n",
    "\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "random_number = round(random.uniform(0, len(bow_corpus)))\n",
    "\n",
    "document_topics = lda.get_document_topics(extract_comment(random_number))\n",
    "sorted_topics = sorted(document_topics, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Topics:\")\n",
    "for topic_id, topic_prob in sorted_topics:\n",
    "    if topic_prob > 0.15:\n",
    "        print(f\"[{topic_id}] Topic: {topic_names[topic_id]} - {topic_prob}\")\n",
    "        # print(f\"[{topic_id}] Topic: {topic_prob}\")\n",
    "        for word_id, p in lda.get_topic_terms(topic_id):\n",
    "            word = dictionary[word_id]\n",
    "            \n",
    "            wprob, wlist =  topic_dict[topic_id][word]\n",
    "            \n",
    "            word_list = []\n",
    "            for string in wlist:\n",
    "                words = string.split()\n",
    "                word_list.extend(words)\n",
    "            \n",
    "            closest_match = find_closest_match(word, word_list)\n",
    "            if closest_match is None: closest_match = min(wlist, key=len) + '*'\n",
    "            if closest_match.isupper(): closest_match = '* ' + word_list[0]\n",
    "            print(closest_match, p)\n",
    "        print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551209a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
