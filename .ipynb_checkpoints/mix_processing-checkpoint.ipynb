{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec3ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word: luck\n",
      "Enter the number of synonyms to generate: 10\n",
      "Synonyms for 'luck':\n",
      "destiny\n",
      "portion\n",
      "fate\n",
      "luck\n",
      "fortune\n",
      "chance\n",
      "hazard\n",
      "circumstances\n",
      "lot\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def generate_synonyms(word, num_synonyms):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    synonyms = list(set(synonyms))  # Remove duplicates\n",
    "    if len(synonyms) <= num_synonyms:\n",
    "        return synonyms\n",
    "    else:\n",
    "        return synonyms[:num_synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a40356c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictonary from corpus length:  572\n",
      "Dictonary from bot length:  1416\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary.load('corpus_dictionary')\n",
    "print('Dictonary from corpus length: ', len(dictionary))\n",
    "dictionary_manual = Dictionary.load('trimmed_dictionary')\n",
    "print('Dictonary from bot length: ', len(dictionary_manual))\n",
    "\n",
    "import json\n",
    "\n",
    "with open('reference_sheet.json', 'r') as file: json_data = file.read()\n",
    "reference_sheet = json.loads(json_data)\n",
    "with open('reference_sheet_manual.json', 'r') as file: json_data = file.read()\n",
    "reference_sheet_manual = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecca6a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'abilities', 'abillity', 'able', 'absolutely', 'absolute', 'absolut', 'absolutly', 'absolution', 'abstract']\n",
      "['power', 'ability', 'power', 'ability', 'capable', 'able-bodied', 'able', 'utterly', 'dead', 'absolutely']\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "old_words = []\n",
    "for token, idx in dictionary.token2id.items(): old_words = old_words + reference_sheet[token]\n",
    "print(old_words[:10])\n",
    "new_words = []\n",
    "for word in old_words: new_words = new_words + generate_synonyms(word, n)\n",
    "print(new_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84410fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "new_reference_sheet = {}\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            word = lemmatize_stemming(token)\n",
    "            if word in new_reference_sheet:\n",
    "                if token not in new_reference_sheet[word]: new_reference_sheet[word].append(token)\n",
    "            else: new_reference_sheet[word] = [token]\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "gensim_words = [preprocess(word) for word in new_words]\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "new_dictionary = gensim.corpora.Dictionary(gensim_words)  \n",
    "new_dictionary.save('addition_dictionary')\n",
    "new_dictionary = Dictionary.load('addition_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bf05824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from collections import defaultdict\n",
    "\n",
    "merged_dict = corpora.Dictionary()\n",
    "\n",
    "merged_dict.merge_with(dictionary)\n",
    "merged_dict.merge_with(new_dictionary)\n",
    "merged_dict.merge_with(dictionary_manual)\n",
    "\n",
    "merged_ref_dict = defaultdict(list)\n",
    "\n",
    "for key, value in reference_sheet.items():\n",
    "    merged_ref_dict[key].extend(value)\n",
    "\n",
    "for key, value in new_reference_sheet.items():\n",
    "    merged_ref_dict[key].extend(value)\n",
    "\n",
    "for key, value in reference_sheet_manual.items():\n",
    "    merged_ref_dict[key].extend(value)\n",
    "    \n",
    "for key, value in merged_ref_dict.items(): merged_ref_dict[key] = list(set(value))\n",
    "merged_ref_dict = dict(merged_ref_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b3f2f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'player', 'play', 'card', 'like', 'fun', 'time', 'mechan'] \n",
      "\n",
      "4740\n",
      "4737 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_words_from_file(filename):\n",
    "    word_list = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "filename = 'words_to_remove.txt'\n",
    "words_to_remove = read_words_from_file(filename)\n",
    "\n",
    "print(words_to_remove, '\\n')\n",
    "print(len(merged_dict))\n",
    "\n",
    "word_ids = [dictionary.token2id[word] for word in words_to_remove if word in dictionary.token2id]\n",
    "merged_dict.filter_tokens(bad_ids=word_ids)\n",
    "merged_dict.compactify()\n",
    "\n",
    "print(len(merged_dict), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9146c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_data = json.dumps(merged_ref_dict)\n",
    "with open('merged_reference_sheet.json', 'w') as file: file.write(json_data)\n",
    "    \n",
    "from gensim.corpora import Dictionary\n",
    "merged_dict.save('merged_dictionary')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a5254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
